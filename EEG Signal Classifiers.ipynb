{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f75c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyxdf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model,model_from_json\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, GRU, LSTM, RNN, Dense, Dropout, Flatten, Activation, Add, ZeroPadding2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras import regularizers as reg\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.initializers import glorot_uniform\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(data, data_t, ref_ival):\n",
    "    '''\n",
    "    Usage:\n",
    "        To baseline the raw data before epoching\n",
    "        data = baseline(data, data_t, ref_ival)\n",
    "    Parameters:\n",
    "        data: a 2D array of data array (n_chan * n_dataPoint)\n",
    "        ref_ival: a two element vector specifying the time interval for which the baseline is calculated [ms]\n",
    "    '''\n",
    "    idxref = (data_t[0] + ref_ival[0] <= data_t) & (data_t <= data_t[0] + (0.001 * ref_ival[1])) # convert ms to s -> 1ms = 0.001s\n",
    "    dataref = np.mean(data[:, idxref], axis=1, keepdims=True)\n",
    "    data = data - dataref\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96739f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_and_test(data_with_label, train_ratio, test_ratio): \n",
    "    '''\n",
    "    Usage:\n",
    "        To shuffle and slice data points with cooresponding labels into training and testing set \n",
    "        training_data, training_label, testing_data, testing_label = prepare_train_and_test(data, label, train_ratio, test_ratio)\n",
    "    Parameters:\n",
    "        data_with_label: a 2D array of data array (n_dataPoint * (label + n_dementions)) with label at row 0\n",
    "        train_ratio, test_ratio: training and testing ratios \n",
    "    '''\n",
    "    # shuffling data with labels\n",
    "    np.random.shuffle(data_with_label)\n",
    "    shuffle_data = data_with_label[:, 1:]\n",
    "    shuffle_label = data_with_label[:, 0]\n",
    "    num_dataPoint = data_with_label.shape[0]\n",
    "    \n",
    "    # slicing\n",
    "    training_data = shuffle_data[0: int(num_dataPoint * train_ratio)]\n",
    "    training_label = shuffle_label[0: int(num_dataPoint * train_ratio)]\n",
    "    testing_data = shuffle_data[int(num_dataPoint * test_ratio)-1: -1]\n",
    "    testing_label = shuffle_label[int(num_dataPoint * test_ratio)-1: -1]\n",
    "    \n",
    "    return training_data, training_label, testing_data, testing_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38392efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_raw(data, chan_name, sfreq, device):\n",
    "    # load electrode locations\n",
    "    enobio_chan_name = []\n",
    "    if device == 'enobio' or device == 'Enobio':\n",
    "        layout = pd.read_csv(r'C:\\Users\\m46ht\\Desktop\\EEGlass\\Enobio_layout.txt', sep = '\\t')\n",
    "    elif device == 'EEGlasses' or device == 'obci':\n",
    "        layout = pd.read_csv(r'C:\\Users\\m46ht\\Desktop\\EEGlass\\EEGlasses_layout.txt', sep = '\\t')\n",
    "    layout.columns = layout.columns.str.strip()\n",
    "    layout[\"Name\"] = layout[\"labels\"].str.strip()\n",
    "    layout = layout.set_index('Name')\n",
    "    layout = layout.to_dict(orient = \"index\")\n",
    "    for channel in layout.keys():\n",
    "        yxz = np.array([-1*layout[channel][\"Y\"], layout[channel][\"X\"], layout[channel][\"Z\"]])\n",
    "        layout[channel] = yxz\n",
    "        enobio_chan_name.append(channel)\n",
    "\n",
    "    if device == 'enobio' or device == 'Enobio':\n",
    "        # remove empty channel EXT\n",
    "        layout.popitem()\n",
    "    montage = mne.channels.make_dig_montage(layout, coord_frame='head')\n",
    "    \n",
    "    # create general info\n",
    "    info = mne.create_info(chan_name, sfreq, ch_types='misc', verbose=False)\n",
    "    data = data / 1e6 # convert units from V to muV\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "\n",
    "    if device == 'enobio' or device == 'Enobio':\n",
    "        raw.set_channel_types({chan_name[i]: 'eeg' for i in range(0, len(chan_name)-1)})\n",
    "        raw.set_montage(montage)\n",
    "    elif device == 'EEGlasses' or device == 'obci':\n",
    "        raw.set_channel_types({chan_name[i]: 'eeg' for i in range(0, len(chan_name))})\n",
    "        raw.set_montage(montage, on_missing='ignore')\n",
    "    raw.load_data()\n",
    "    # mark reference electrode as bad\n",
    "    #raw.info['bads'] = ['EXT']\n",
    "    raw.pick_types(meg=False, eeg=True, ref_meg=False)\n",
    "\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICA_filter(raw, freqs_sig = [1, 90], n_components=20):  \n",
    "    raw.load_data()\n",
    "    filt_raw = raw.copy().filter(l_freq=freqs_sig[0], h_freq=freqs_sig[1])\n",
    "    ica = mne.preprocessing.ICA(n_components, max_iter='auto', random_state=97, method='fastica')\n",
    "    ica.fit(inst=filt_raw)\n",
    "    return ica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68747624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ICA_filter(ica, raw):\n",
    "    raw.load_data()\n",
    "    filtered_raw = ica.apply(inst = raw, verbose = False)\n",
    "    return filtered_raw.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psd(signal, fmin = 1., fmax = 90.):\n",
    "    tmin = 0.1\n",
    "    tmax = 5.\n",
    "    sfreq = 8000\n",
    "    psd, freqs = mne.time_frequency.psd_array_welch(\n",
    "        signal, sfreq=sfreq, \n",
    "        n_fft=int(sfreq * (tmax - tmin)), n_overlap=0,\n",
    "        n_per_seg=int(sfreq * (tmax - tmin)),fmin=fmin,\n",
    "        fmax=fmax,window='boxcar',verbose=False)\n",
    "    return psd, freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe316969",
   "metadata": {},
   "outputs": [],
   "source": [
    "enobio_chan_name = []\n",
    "layout = pd.read_csv(r'C:\\Users\\m46ht\\Desktop\\EEGlass\\Enobio_layout.txt', sep = '\\t')\n",
    "layout.columns = layout.columns.str.strip()\n",
    "layout[\"Name\"] = layout[\"labels\"].str.strip()\n",
    "layout = layout.set_index('Name')\n",
    "layout = layout.to_dict(orient = \"index\")\n",
    "for channel in layout.keys():\n",
    "    #if channel == 'EXT':\n",
    "    #    continue\n",
    "    yxz = np.array([-1*layout[channel][\"Y\"], layout[channel][\"X\"], layout[channel][\"Z\"]])\n",
    "    layout[channel] = yxz\n",
    "    enobio_chan_name.append(channel)\n",
    "\n",
    "# remove empty channel EXT\n",
    "layout.popitem()\n",
    "montage = mne.channels.make_dig_montage(layout, coord_frame='head')\n",
    "montage_plot = mne.viz.plot_montage(montage, kind = 'topomap', scale_factor = 0.05, sphere=(0, 0, 0, 125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb1700",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EEGlasses_chan_name = []\n",
    "layout = pd.read_csv(r'C:\\Users\\m46ht\\Desktop\\EEGlass\\EEGlasses_layout.txt', sep = '\\t')\n",
    "layout.columns = layout.columns.str.strip()\n",
    "layout[\"Name\"] = layout[\"labels\"].str.strip()\n",
    "layout = layout.set_index('Name')\n",
    "layout = layout.to_dict(orient = \"index\")\n",
    "for channel in layout.keys():\n",
    "    #if channel == 'EXT':\n",
    "    #    continue\n",
    "    yxz = np.array([-1*layout[channel][\"Y\"], layout[channel][\"X\"], layout[channel][\"Z\"]])\n",
    "    layout[channel] = yxz\n",
    "    EEGlasses_chan_name.append(channel)\n",
    "\n",
    "# remove empty channel EXT\n",
    "layout.popitem()\n",
    "montage = mne.channels.make_dig_montage(layout, coord_frame='head')\n",
    "montage_plot = mne.viz.plot_montage(montage, kind = 'topomap', scale_factor = 0.05, sphere=(0, 0, 0, 125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea35c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeepochs(X, fs, markers, timestamp):\n",
    "    '''\n",
    "    Usage:\n",
    "        makeepochs(X, fs, markers, ival)\n",
    "    Parameters:\n",
    "        X: 2D array of multi-channel timeseries (channels x samples) \n",
    "        fs: sampling frequency [Hz]\n",
    "        markers: marker positions [sa]\n",
    "        timestamp: time points for ongoing data (in s)\n",
    "    Returns:\n",
    "        epo: a 3D array of segmented signals (samples x channels x epochs)\n",
    "        epo_t: a 1D array of time points of epochs relative to marker (in s)\n",
    "    '''\n",
    "    event_time_index = []\n",
    "    epo_t = []\n",
    "    #epo = np.array([])\n",
    "    epo = []\n",
    "    # 0-scaled time points for ongoing data \n",
    "    all_time_index = np.array([range(0, int(np.ceil((timestamp[-1] - timestamp[0])*fs))+1)])\n",
    "    T = all_time_index.shape[1]\n",
    "    nEvents = len(markers)\n",
    "    markers_diff = np.diff(markers).tolist()\n",
    "    nChans = X.shape[0]\n",
    "    \n",
    "    # generate the time points that events happend\n",
    "    for i in range(len(markers_diff)):\n",
    "        event_time_index.append(int(sum(markers_diff[0:i]) * fs))\n",
    "    event_time_index.append(all_time_index[0][-1] + 1)\n",
    "\n",
    "    #epo = X[:,idx].T.reshape(T, nEvents, nChans)\n",
    "    #epo = np.transpose(epo, (0,2,1))\n",
    "    #epo_t = np.linspace(0, timestamp[-1] - timestamp[0], T)\n",
    "    #return epo, epo_t\n",
    "\n",
    "    # slicing event epoch depending on markers' indice\n",
    "    for j in range(len(event_time_index)): \n",
    "        if j != len(event_time_index)-1:\n",
    "            event = X[:, event_time_index[j]:event_time_index[j+1]]\n",
    "            #epo = np.append(epo, event, axis=0)\n",
    "            epo.append(event)\n",
    "            event_time = timestamp[j]\n",
    "            epo_t.append(event_time)\n",
    "\n",
    "    return epo, epo_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143162f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epochs(X, time_series, mrk_pos, ival):\n",
    "    '''\n",
    "    Usage:\n",
    "        makeepochs(X, fs, mrk_pos, ival)\n",
    "    Parameters:\n",
    "        X: 2D array of multi-channel timeseries (channels x samples) \n",
    "        fs: sampling frequency [Hz]\n",
    "        mrk_pos: marker positions [sa]\n",
    "        ival: a two element vector giving the time interval relative to markers (in ms)\n",
    "    Returns:\n",
    "        epo: a 3D array of segmented signals (samples x channels x epochs)\n",
    "        epo_t: a 1D array of time points of epochs relative to marker (in ms)\n",
    "    '''\n",
    "    time = np.array([range(np.int(np.floor(ival[0]*fs/1000)), \n",
    "                           np.int(np.ceil(ival[1]*fs/1000))+1)])\n",
    "    T = time_series.shape[1]\n",
    "    nEvents = len(mrk_pos)\n",
    "    nChans = X.shape[0]\n",
    "    idx = (time.T+np.array([mrk_pos])).reshape(1, T*nEvents)    \n",
    "    epo = X[:,idx].T.reshape(T, nEvents, nChans)\n",
    "    epo = np.transpose(epo, (0,2,1))\n",
    "    epo_t = np.linspace(ival[0], ival[1], T)\n",
    "    return epo, epo_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b923c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "taskType_list = ['2-back', 'Backward1', 'Backward2', 'Stroop1', 'Stroop2', 'Stroop3', 'Stroop4']\n",
    "label_encoder = LabelEncoder()\n",
    "taskType_integer_encoded = label_encoder.fit_transform(taskType_list)\n",
    "# convert to one_hot\n",
    "taskType_one_hot_encoded = np.zeros((taskType_integer_encoded.size, taskType_integer_encoded.max()+1))\n",
    "taskType_one_hot_encoded[np.arange(taskType_integer_encoded.size),taskType_integer_encoded] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91837cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_participant_data(parti_ind = [1], one_hot_encoding = True, scaling = True):\n",
    "    #taskType_list = ['2-back', 'Backward1', 'Backward2', 'RestingClosed', 'RestingOpen', 'Stroop1', 'Stroop2', 'Stroop3', 'Stroop4']\n",
    "    taskType_list = ['2-back', 'Backward1', 'Backward2', 'Stroop1', 'Stroop2', 'Stroop3', 'Stroop4']\n",
    "    #label_encoder = LabelEncoder()\n",
    "    #taskType_integer_encoded = label_encoder.fit_transform(taskType_list)\n",
    "    taskType_integer_encoded = np.array([0, 1, 1, 2, 2, 2, 2]) # simplified into 3 classes: 2back, backward and stroop\n",
    "    # convert to one_hot\n",
    "    taskType_one_hot_encoded = np.zeros((taskType_integer_encoded.size, taskType_integer_encoded.max()+1))\n",
    "    taskType_one_hot_encoded[np.arange(taskType_integer_encoded.size),taskType_integer_encoded] = 1 \n",
    "    exclude = []\n",
    "    participant_data_enobio = pd.DataFrame()\n",
    "    participant_label_enobio = []\n",
    "    participant_data_eegGlasses = pd.DataFrame()\n",
    "    participant_label_eegGlasses = []\n",
    "    # scaling into unit variance for ML classifiers\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    for participantID in parti_ind:\n",
    "        if participantID in exclude:\n",
    "            print(\"Contains dead channels... Please re-enter...\")\n",
    "            return [], [], [], []\n",
    "        else:\n",
    "            if one_hot_encoding:\n",
    "                for taskType, taskType_one_hot in zip(taskType_list, taskType_one_hot_encoded): \n",
    "                    # read csv data \n",
    "                    df_enobio = pd.read_csv(r\"C:\\Users\\m46ht\\Desktop\\EEGlass\\enobio_psd_results\\parti\"\n",
    "                                            + str(participantID) + \"\\parti_\" + str(participantID) \n",
    "                                            + \"_\" + taskType + \"_psd.csv\")\n",
    "                    df_eegGlasses = pd.read_csv(r\"C:\\Users\\m46ht\\Desktop\\EEGlass\\eegGlasses_psd_results\\parti\" \n",
    "                                                + str(participantID) + \"\\parti_\" + str(participantID) \n",
    "                                                + \"_\" + taskType + \"_psd.csv\")\n",
    "                    # drop the last row -- we have 19 active channels in each task \n",
    "                    df_enobio.drop(df_enobio.tail(1).index, inplace=True)\n",
    "                    participant_data_enobio = participant_data_enobio.append(df_enobio)\n",
    "                    participant_data_eegGlasses = participant_data_eegGlasses.append(df_eegGlasses)\n",
    "                    for i in range(len(df_enobio)):\n",
    "                        participant_label_enobio.append(taskType_one_hot)\n",
    "                    for j in range(len(df_eegGlasses)):\n",
    "                        participant_label_eegGlasses.append(taskType_one_hot)\n",
    "            else: \n",
    "                for taskType, taskType_int in zip(taskType_list, taskType_integer_encoded): \n",
    "                    df_enobio = pd.read_csv(r\"C:\\Users\\m46ht\\Desktop\\EEGlass\\enobio_psd_results\\parti\" \n",
    "                                            + str(participantID) + \"\\parti_\" + str(participantID) \n",
    "                                            + \"_\" + taskType + \"_psd.csv\")\n",
    "                    df_eegGlasses = pd.read_csv(r\"C:\\Users\\m46ht\\Desktop\\EEGlass\\eegGlasses_psd_results\\parti\" \n",
    "                                                + str(participantID) + \"\\parti_\" + str(participantID) \n",
    "                                                + \"_\" + taskType + \"_psd.csv\")\n",
    "                    df_enobio.drop(df_enobio.tail(1).index, inplace=True)\n",
    "                    participant_data_enobio = participant_data_enobio.append([df_enobio])\n",
    "                    participant_data_eegGlasses = participant_data_eegGlasses.append([df_eegGlasses])\n",
    "                    for i in range(len(df_enobio)):\n",
    "                        participant_label_enobio.append([taskType_int])\n",
    "                    for j in range(len(df_eegGlasses)):\n",
    "                        participant_label_eegGlasses.append([taskType_int])\n",
    "\n",
    "    participant_data_enobio = np.asarray(participant_data_enobio)\n",
    "    participant_label_enobio = np.asarray(participant_label_enobio)\n",
    "    participant_data_eegGlasses = np.asarray(participant_data_eegGlasses)\n",
    "    participant_label_eegGlasses = np.asarray(participant_label_eegGlasses)\n",
    "    \n",
    "    if scaling: \n",
    "        participant_data_enobio = min_max_scaler.fit_transform(participant_data_enobio)\n",
    "        participant_data_eegGlasses = min_max_scaler.fit_transform(participant_data_eegGlasses)\n",
    "        return participant_data_enobio.tolist(), participant_label_enobio.tolist(), participant_data_eegGlasses.tolist(), participant_label_eegGlasses.tolist()\n",
    "    else: \n",
    "        return participant_data_enobio.tolist(), participant_label_enobio.tolist(), participant_data_eegGlasses.tolist(), participant_label_eegGlasses.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908cd808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topomap(raw, l_freq, h_freq, plot_range, task):\n",
    "    range_index = [int(raw.info['sfreq'] * (plot_range[0] - l_freq)), int(raw.info['sfreq'] * (plot_range[1] - l_freq))]\n",
    "    data = np.mean(raw.get_data()[:, range_index[0] : range_index[1]], axis = 1)\n",
    "    print(\"Topography for averaged value from \" + str(plot_range) + \" Hz\")\n",
    "    fig, ax = plt.subplots()\n",
    "    mne.viz.plot_topomap(data, raw.info, outlines='head', extrapolate = 'head', sphere=(0, 0, 0, 125), axes=ax,\n",
    "                     show=False)\n",
    "    # add titles\n",
    "    #ax.set_title(str(plot_range[0]) + \"Hz to \" + str(plot_range[1]) + \"Hz -- \" + str(task), fontweight='bold')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4101920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_participants = np.arange(1, 35)\n",
    "enobio_data = np.array([])\n",
    "enbobio_label = []\n",
    "fmin = 30.\n",
    "fmax = 100.\n",
    "ref_ival = [0, 500] # use 0 to 100ms as reference for baselining\n",
    "for participantID in all_participants:\n",
    "    for taskType, taskType_integer in zip(taskType_list, taskType_integer_encoded): \n",
    "        xdf_name = r\"C:\\Users\\m46ht\\Desktop\\EEGlass\\EEGlass-MasterDataset\\EEGlass-MasterDataset\\Participant-\" + str(participantID) + \"\\EEG\\\\\" + \"\" + taskType + \"\\\\block_Default.xdf\"\n",
    "        stream, header = pyxdf.load_xdf(xdf_name)\n",
    "        for item in stream:\n",
    "\n",
    "            # collect data from enobio 20 channels \n",
    "            if item['info']['name'][0] == 'Enobio' and item['info']['type'][0] == 'EEG' and item['info']['channel_count'][0] == '20' and item[\"time_series\"].T[0:3].shape[1] != 1:\n",
    "                enobio_time = item[\"time_stamps\"]                \n",
    "                temp_enobio_data = item[\"time_series\"].T\n",
    "                #enobio_chan_type = [chan_type['type'] for chan_type in item['info']['desc'][0]['channel']]\n",
    "                #enobio_chan_type = [chan_type[0] for chan_type in enobio_chan_type]\n",
    "                # get sampling freq\n",
    "                enobio_sfreq = float(item[\"info\"][\"nominal_srate\"][0])\n",
    "\n",
    "                # baselining\n",
    "                temp_enobio_data = baseline(temp_enobio_data, enobio_time, ref_ival)\n",
    "                temp_enobio_raw = convert_to_raw(temp_enobio_data, enobio_chan_name, enobio_sfreq, device = 'Enobio')\n",
    "                enobio_psd = temp_enobio_raw.plot_psd(average = False, fmin = fmin, fmax = fmax)\n",
    "                enobio_psd.savefig(r\"C:\\Users\\m46ht\\Desktop\\EEGlass\\enobio_psd_results\\parti\" + str(participantID) + \"\\parti_\" + str(participantID) + \"_\" + taskType + \"_psd.png\", dpi = 150, bbox_inches='tight') \n",
    "                \n",
    "                # remove refrence channel -- they are all 0s\n",
    "                #removed_refchan_data = rereferenced_raw.get_data()[~np.all(rereferenced_raw.get_data() == 0, axis=1)]\n",
    "                removed_refchan_data = temp_enobio_raw.get_data()[~np.all(temp_enobio_raw.get_data() == 0, axis=1)]\n",
    "                \n",
    "                # conpute psd and their freqs \n",
    "                temp_enobio_psd, temp_enobio_freqs = compute_psd(removed_refchan_data, fmin = fmin, fmax = fmax)\n",
    "                \n",
    "                # name of csv file \n",
    "                temp_filename = r\"C:\\Users\\m46ht\\Desktop\\EEGlass\\enobio_psd_results\\parti\" + str(participantID) + \"\\parti_\" + str(participantID) + \"_\" + taskType + \"_psd.csv\"\n",
    "                df_psd = pd.DataFrame(temp_enobio_psd)\n",
    "                df_psd.to_csv(temp_filename, index=False)\n",
    "                \n",
    "                \n",
    "            # collect data from open bci 3 channels \n",
    "            elif (item['info']['name'][0] == 'obci_eeg1' or item['info']['name'][0] == 'openbci_eeg') and item['info']['type'][0] == 'EEG' and item['info']['channel_count'][0] == '8' and item[\"time_series\"].T[0:3].shape[1] != 1:\n",
    " \n",
    "                temp_obci_data = item[\"time_series\"].T[0:3]\n",
    "                obci_time = item[\"time_stamps\"]\n",
    "                obci_chan_name = ['Nz', 'TP10', 'TP9']\n",
    "                obci_sfreq = float(item[\"info\"][\"nominal_srate\"][0])\n",
    " \n",
    "                temp_obci_data = baseline(temp_obci_data, obci_time, ref_ival)\n",
    "                # for visualization only\n",
    "                temp_obci_data = temp_obci_data / 700\n",
    "                temp_obci_raw = convert_to_raw(temp_obci_data, obci_chan_name, obci_sfreq, device = 'EEGlasses')\n",
    "\n",
    "                # collect psd plots\n",
    "                #obci_psd = temp_obci_raw.plot_psd(average = False, fmin = fmin, fmax = fmax, exclude = ['Nz']) \n",
    "                obci_psd = temp_obci_raw.plot_psd(average = False, fmin = fmin, fmax = fmax) \n",
    "                obci_psd.savefig(r\"C:\\Users\\m46ht\\Desktop\\EEGlass\\eegGlasses_psd_results\\parti\" + str(participantID) + \"\\parti_\" + str(participantID) + \"_\" + taskType + \"_psd.png\", dpi = 150, bbox_inches='tight') \n",
    "\n",
    "                # remove refrence channel -- they are all 0s\n",
    "                #removed_refchan_data = rereferenced_raw.get_data()[~np.all(rereferenced_raw.get_data() == 0, axis=1)]\n",
    "                removed_refchan_data = temp_obci_raw.get_data()[~np.all(temp_obci_raw.get_data() == 0, axis=1)]\n",
    "                # conpute psd and their freqs \n",
    "                temp_obci_psd, temp_obci_freqs = compute_psd(removed_refchan_data, fmin = fmin, fmax = fmax)\n",
    "                temp_filename = r\"C:\\Users\\m46ht\\Desktop\\EEGlass\\eegGlasses_psd_results\\parti\" + str(participantID) + \"\\parti_\" + str(participantID) + \"_\" + taskType + \"_psd.csv\"\n",
    "                df_psd = pd.DataFrame(temp_obci_psd)\n",
    "                df_psd.to_csv(temp_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(train_X, train_label, conv_layers=1, conv_size=344, filter_size=1, fc_layers=2, fc_sizes=(4096,2048),\n",
    "        dropout=0.5, pool_size=2, init='he_uniform', act='relu', optim='adam', pool=True,\n",
    "        reg = reg.l2(0.05)):\n",
    "    classifier = tf.keras.models.Sequential()\n",
    "    for i in range(conv_layers):\n",
    "        classifier.add(Conv1D(conv_size, filter_size, activation='relu',input_shape=(1, conv_size), kernel_initializer=init, kernel_regularizer=reg))\n",
    "        classifier.add(BatchNormalization())\n",
    "        if pool:\n",
    "            classifier.add(MaxPooling1D(pool_size = 1))\n",
    "    #classifier.add(Flatten())\n",
    "    for j in range(fc_layers):\n",
    "        classifier.add(Dense(fc_sizes, activation = act,kernel_initializer=init,kernel_regularizer=reg))\n",
    "    #    classifier.add(Dropout(dropout))\n",
    "    classifier.add(Dense(3, activation = 'softmax',kernel_initializer=init))\n",
    "    classifier.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    classifier.fit(train_X, train_label, batch_size= 256, epochs= 15, verbose = True) \n",
    "    #classifier.summary()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(X, label, X_cat, label_cat):\n",
    "    if len(X_cat) == 0:\n",
    "        X_cat = X\n",
    "        label_cat = label\n",
    "    else:\n",
    "        X_cat = np.concatenate((X_cat, X), axis = 0)\n",
    "        label_cat = np.concatenate((label_cat, label), axis = 0)\n",
    "    return X_cat, label_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_result(participants, excluded_participants, individual_train = True, test_item = 'Enobio', test_model = 'svm', scaling = False):\n",
    "    acc_val_list = []\n",
    "    # create a list of random seeds for getting reasonable acc vals \n",
    "    # -- (K-fold val like but due to short in data, it is hard to implement such)\n",
    "    rand_seed_list = [1]\n",
    "    \n",
    "    if test_model == 'svm':\n",
    "        svm_classifier = svm.SVC(kernel='poly')\n",
    "        # train using data from indivudual seperately\n",
    "        if individual_train:\n",
    "            temp_acc_list = []\n",
    "            for rand_seed in rand_seed_list:\n",
    "                # initiation for combined training and testing \n",
    "                train_X_cat = np.array(())\n",
    "                train_label_cat = np.array(())\n",
    "                valid_X_cat = np.array(())\n",
    "                valid_label_cat = np.array(())\n",
    "                # train part\n",
    "                for person in participants:\n",
    "                    participant_data_enobio, participant_label_enobio, participant_data_eegGlasses, participant_label_eegGlasses = load_participant_data(parti_ind = [person], one_hot_encoding = False, scaling = scaling)\n",
    "                    if test_item == 'Enobio' or test_item == 'enobio':\n",
    "                        train_X, train_label = participant_data_enobio, participant_label_enobio\n",
    "                    elif test_item == 'EEGlasses':\n",
    "                        train_X, train_label = participant_data_eegGlasses, participant_label_eegGlasses\n",
    "                    train_X_cat, train_label_cat = concatenate(train_X, train_label, train_X_cat, train_label_cat)\n",
    "\n",
    "                # valid/test part\n",
    "                for person in excluded_participants:\n",
    "                    excluded_participant_data_enobio, excluded_participant_label_enobio, excluded_participant_data_eegGlasses, excluded_participant_label_eegGlasses = load_participant_data(parti_ind = [person], one_hot_encoding = False, scaling = scaling)\n",
    "                    if test_item == 'Enobio' or test_item == 'enobio':\n",
    "                        valid_X, valid_label = excluded_participant_data_enobio, excluded_participant_label_enobio\n",
    "                    elif test_item == 'EEGlasses':\n",
    "                        valid_X, valid_label = excluded_participant_data_eegGlasses, excluded_participant_label_eegGlasses\n",
    "                    valid_X_cat, valid_label_cat = concatenate(valid_X, valid_label, valid_X_cat, valid_label_cat)\n",
    "                \n",
    "                # use concatenated data to fit classifier \n",
    "                svm_classifier.fit(train_X_cat,train_label_cat)\n",
    "\n",
    "                yhat = svm_classifier.predict(valid_X_cat)\n",
    "                acc_val = accuracy_score(valid_label_cat,yhat)\n",
    "            acc_val_list = acc_val\n",
    "    \n",
    "    elif test_model == 'knn':\n",
    "        knn_classifier = KNeighborsClassifier(n_neighbors = 1, weights = 'distance')\n",
    "        # train using data from indivudual seperately\n",
    "        if individual_train:\n",
    "            temp_acc_list = []\n",
    "            for rand_seed in rand_seed_list:\n",
    "                # initiation for combined training and testing \n",
    "                train_X_cat = np.array(())\n",
    "                train_label_cat = np.array(())\n",
    "                valid_X_cat = np.array(())\n",
    "                valid_label_cat = np.array(())\n",
    "                # train part\n",
    "                for person in participants:\n",
    "                    participant_data_enobio, participant_label_enobio, participant_data_eegGlasses, participant_label_eegGlasses = load_participant_data(parti_ind = [person], one_hot_encoding = False, scaling = scaling)\n",
    "                    if test_item == 'Enobio' or test_item == 'enobio':\n",
    "                        train_X, train_label = participant_data_enobio, participant_label_enobio\n",
    "                    elif test_item == 'EEGlasses':\n",
    "                        train_X, train_label = participant_data_eegGlasses, participant_label_eegGlasses\n",
    "                    train_X_cat, train_label_cat = concatenate(train_X, train_label, train_X_cat, train_label_cat)\n",
    "                \n",
    "                # valid\\test part\n",
    "                for person in excluded_participants:\n",
    "                    excluded_participant_data_enobio, excluded_participant_label_enobio, excluded_participant_data_eegGlasses, excluded_participant_label_eegGlasses = load_participant_data(parti_ind = [person], one_hot_encoding = False, scaling = scaling)\n",
    "                    if test_item == 'Enobio' or test_item == 'enobio':\n",
    "                        valid_X, valid_label = excluded_participant_data_enobio, excluded_participant_label_enobio\n",
    "                    elif test_item == 'EEGlasses':\n",
    "                        valid_X, valid_label = excluded_participant_data_eegGlasses, excluded_participant_label_eegGlasses\n",
    "                    valid_X_cat, valid_label_cat = concatenate(valid_X, valid_label, valid_X_cat, valid_label_cat)\n",
    "                     \n",
    "                # use concatenated data to fit classifier \n",
    "                knn_classifier.fit(train_X_cat, train_label_cat)\n",
    "                num_ValdataPoint = len(valid_X_cat[0])\n",
    "                for i in range(1, len(excluded_participants)+1):\n",
    "                    #valid_X = valid_X_cat[int(num_ValdataPoint / len(excluded_participants) * (i-1)) : int(num_ValdataPoint / len(participants) * i)]\n",
    "                    #valid_label = valid_label_cat[int(num_ValdataPoint / len(excluded_participants) * (i-1)) : int(num_ValdataPoint / len(excluded_participants) * i)]\n",
    "                    yhat = knn_classifier.predict(valid_X_cat)\n",
    "                    acc_val = accuracy_score(valid_label_cat, yhat)\n",
    "                    temp_acc_list.append(acc_val)\n",
    "                acc_val_list = temp_acc_list\n",
    "                \n",
    "                \n",
    "    elif test_model == 'XGBoost':\n",
    "        XGB_model = XGBClassifier()\n",
    "        # train using data from indivudual seperately\n",
    "        if individual_train:\n",
    "            temp_acc_list = []\n",
    "            for rand_seed in rand_seed_list:\n",
    "                # initiation for combined training and testing \n",
    "                train_X_cat = np.array(())\n",
    "                train_label_cat = np.array(())\n",
    "                valid_X_cat = np.array(())\n",
    "                valid_label_cat = np.array(())\n",
    "                # train part\n",
    "                for person in participants:\n",
    "                    participant_data_enobio, participant_label_enobio, participant_data_eegGlasses, participant_label_eegGlasses = load_participant_data(parti_ind = [person], one_hot_encoding = False, scaling = scaling)\n",
    "                    if test_item == 'Enobio' or test_item == 'enobio':\n",
    "                        train_X, train_label = participant_data_enobio, participant_label_enobio\n",
    "                    elif test_item == 'EEGlasses':\n",
    "                        train_X, train_label = participant_data_eegGlasses, participant_label_eegGlasses\n",
    "                    train_X_cat, train_label_cat = concatenate(train_X, train_label, train_X_cat, train_label_cat)\n",
    "                \n",
    "                # valid\\test part\n",
    "                for person in excluded_participants:\n",
    "                    excluded_participant_data_enobio, excluded_participant_label_enobio, excluded_participant_data_eegGlasses, excluded_participant_label_eegGlasses = load_participant_data(parti_ind = [person], one_hot_encoding = False, scaling = scaling)\n",
    "                    if test_item == 'Enobio' or test_item == 'enobio':\n",
    "                        valid_X, valid_label = excluded_participant_data_enobio, excluded_participant_label_enobio\n",
    "                    elif test_item == 'EEGlasses':\n",
    "                        valid_X, valid_label = excluded_participant_data_eegGlasses, excluded_participant_label_eegGlasses\n",
    "                    valid_X_cat, valid_label_cat = concatenate(valid_X, valid_label, valid_X_cat, valid_label_cat)\n",
    "                    \n",
    "                    \n",
    "                # use concatenated data to fit classifier \n",
    "                XGB_model.fit(train_X_cat, train_label_cat)\n",
    "                num_ValdataPoint = len(valid_X_cat[0])\n",
    "                for i in range(1, len(excluded_participants)+1):\n",
    "                    #valid_X = valid_X_cat[int(num_ValdataPoint / len(participants) * (i-1)) : int(num_ValdataPoint / len(participants) * i)]\n",
    "                    #valid_label = valid_label_cat[int(num_ValdataPoint / len(participants) * (i-1)) : int(num_ValdataPoint / len(participants) * i)]\n",
    "                    yhat = XGB_model.predict(valid_X_cat)\n",
    "                    acc_val = accuracy_score(valid_label_cat, yhat)\n",
    "                    temp_acc_list.append(acc_val)\n",
    "                acc_val_list = temp_acc_list\n",
    "\n",
    "    elif test_model == 'cnn':\n",
    "        # train using data from indivudual seperately\n",
    "        if individual_train:\n",
    "            temp_acc_list = []\n",
    "            for rand_seed in rand_seed_list:\n",
    "                # initiation for combined training and testing \n",
    "                train_X_cat = np.array(())\n",
    "                train_label_cat = np.array(())\n",
    "                valid_X_cat = np.array(())\n",
    "                valid_label_cat = np.array(())\n",
    "                # train part\n",
    "                for person in participants:\n",
    "                    participant_data_enobio, participant_label_enobio, participant_data_eegGlasses, participant_label_eegGlasses = load_participant_data(parti_ind = [person], one_hot_encoding = True, scaling = scaling)\n",
    "                    if test_item == 'Enobio' or test_item == 'enobio':\n",
    "                        train_X, train_label = participant_data_enobio, participant_label_enobio\n",
    "                    elif test_item == 'EEGlasses':\n",
    "                        train_X, train_label = participant_data_eegGlasses, participant_label_eegGlasses\n",
    "                    train_X_cat, train_label_cat = concatenate(train_X, train_label, train_X_cat, train_label_cat)\n",
    "                    \n",
    "                for person in excluded_participants:\n",
    "                    excluded_participant_data_enobio, excluded_participant_label_enobio, excluded_participant_data_eegGlasses, excluded_participant_label_eegGlasses = load_participant_data(parti_ind = [person], one_hot_encoding = True, scaling = scaling)\n",
    "                    if test_item == 'Enobio' or test_item == 'enobio':\n",
    "                        valid_X, valid_label = excluded_participant_data_enobio, excluded_participant_label_enobio\n",
    "                    elif test_item == 'EEGlasses':\n",
    "                        valid_X, valid_label = excluded_participant_data_eegGlasses, excluded_participant_label_eegGlasses\n",
    "                    valid_X_cat, valid_label_cat = concatenate(valid_X, valid_label, valid_X_cat, valid_label_cat)\n",
    "                    \n",
    "                # use concatenated data to fit classifier \n",
    "                num_ValdataPoint = len(valid_X_cat[0])\n",
    "                train_X_cat = np.expand_dims(train_X_cat, axis = 1)\n",
    "                train_label_cat = np.expand_dims(train_label_cat, axis = 1)\n",
    "                classifier = cnn(train_X_cat, train_label_cat, conv_layers=3, fc_layers=2, conv_size=344, filter_size=1, fc_sizes=344)\n",
    "                for i in range(1, len(excluded_participants)+1):\n",
    "                    \n",
    "                    valid_X = valid_X_cat[int(num_ValdataPoint / len(excluded_participants) * (i-1)) : int(num_ValdataPoint / len(excluded_participants) * i)]\n",
    "                    valid_label = valid_label_cat[int(num_ValdataPoint / len(excluded_participants) * (i-1)) : int(num_ValdataPoint / len(excluded_participants) * i)]\n",
    "                    valid_X = np.expand_dims(valid_X, axis = 1)\n",
    "                    valid_label = np.expand_dims(valid_label, axis = 1)\n",
    "                    yhat = classifier.predict(valid_X)\n",
    "                    valid_label = np.squeeze(valid_label, axis = 1)\n",
    "                    yhat = np.squeeze(yhat, axis = 1)\n",
    "                    yhat_clean = np.zeros(valid_label.shape)\n",
    "                    yhat_clean[np.arange(yhat.shape[0]), np.argmax(yhat, axis=1)] = 1\n",
    "                    acc_val = accuracy_score(np.argmax(valid_label, axis=1), np.argmax(yhat_clean, axis=1))\n",
    "                    temp_acc_list.append(acc_val)                    \n",
    "\n",
    "                    #acc_val = accuracy_score(valid_label, yhat)\n",
    "                    #temp_acc_list.append(acc_val)\n",
    "                acc_val_list = temp_acc_list\n",
    "    return participants, acc_val_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
